# üîß CRITICAL FIXES IMPLEMENTATION GUIDE - PART 2 (Issues #11-20)

## **FIX #11: Add Dead Letter Queue for Failed Messages**

### **Problem:**
Issue #143 - Malformed messages just logged and skipped, no investigation possible.

### **Solution:**

```java
package com.kotsin.execution.config;

import org.apache.kafka.clients.producer.ProducerConfig;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.util.backoff.FixedBackOff;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class DeadLetterQueueConfig {

    @Bean
    public KafkaTemplate<String, String> dlqKafkaTemplate() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                  org.apache.kafka.common.serialization.StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                  org.apache.kafka.common.serialization.StringSerializer.class);

        ProducerFactory<String, String> pf = new DefaultKafkaProducerFactory<>(props);
        return new KafkaTemplate<>(pf);
    }

    @Bean
    public DefaultErrorHandler errorHandler(KafkaTemplate<String, String> dlqTemplate) {
        // üõ°Ô∏è CRITICAL FIX #143: Send failed messages to DLQ
        DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(
            dlqTemplate,
            (record, ex) -> {
                // DLQ topic name: original-topic + ".DLT" (Dead Letter Topic)
                String dlqTopic = record.topic() + ".DLT";
                log.error("Publishing to DLQ: topic={} partition={} offset={} error={}",
                          dlqTopic, record.partition(), record.offset(), ex.getMessage());
                return new org.apache.kafka.common.TopicPartition(dlqTopic, record.partition());
            }
        );

        DefaultErrorHandler errorHandler = new DefaultErrorHandler(
            recoverer,
            new FixedBackOff(1000L, 3L)  // 3 retries with 1s delay
        );

        // Don't retry deserialization errors (they'll always fail)
        errorHandler.addNotRetryableExceptions(
            org.springframework.kafka.support.serializer.DeserializationException.class,
            org.apache.kafka.common.errors.SerializationException.class
        );

        return errorHandler;
    }
}
```

### **DLQ Consumer for Investigation:**

```java
@Service
@Slf4j
public class DeadLetterQueueConsumer {

    @KafkaListener(topics = "trading-signals.DLT", groupId = "dlq-investigator")
    public void consumeDeadLetter(ConsumerRecord<String, String> record,
                                  @Header(KafkaHeaders.EXCEPTION_MESSAGE) String exceptionMessage) {
        log.error("üö® Dead Letter Message Received:");
        log.error("  Original Topic: trading-signals");
        log.error("  Partition: {}", record.partition());
        log.error("  Offset: {}", record.offset());
        log.error("  Key: {}", record.key());
        log.error("  Value: {}", record.value());
        log.error("  Exception: {}", exceptionMessage);

        // Store in MongoDB for analysis
        DeadLetterRecord dlr = DeadLetterRecord.builder()
            .topic("trading-signals")
            .partition(record.partition())
            .offset(record.offset())
            .key(record.key())
            .value(record.value())
            .exceptionMessage(exceptionMessage)
            .timestamp(LocalDateTime.now())
            .build();

        deadLetterRepository.save(dlr);

        // Send alert to team
        telegramNotificationService.sendDeveloperAlert(
            "üö® Dead Letter Queue Alert",
            String.format("Topic: trading-signals\nOffset: %d\nError: %s",
                         record.offset(), exceptionMessage)
        );
    }
}
```

---

## **FIX #12: Fix OHLC Candle Building (Track Min/Max Across Ticks)**

### **Problem:**
Issue #19 - Multiple ticks in same minute ‚Üí last tick overwrites OHLC.

### **Current Broken Code:**

```java
// LiveMarketDataConsumer.java
Candlestick bar = new Candlestick();
bar.setOpen(marketData.getOpenRate());  // ‚ùå Uses last tick's open!
bar.setHigh(marketData.getHigh());      // ‚ùå Uses last tick's high!
bar.setLow(marketData.getLow());        // ‚ùå Uses last tick's low!
bar.setClose(marketData.getLastRate()); // ‚úÖ This is correct
```

### **Fixed Code - Proper OHLC Aggregation:**

```java
@Component
@Slf4j
@RequiredArgsConstructor
public class LiveMarketDataConsumer {

    private final com.kotsin.execution.logic.TradeManager tradeManager;

    // üõ°Ô∏è CRITICAL FIX #19: Track OHLC across all ticks in the same minute
    private final Map<Integer, CandleBuilder> candleBuilders = new ConcurrentHashMap<>();

    @KafkaListener(
        topics = "forwardtesting-data",
        containerFactory = "marketDataKafkaListenerContainerFactory"
    )
    public void consumeMarketData(@Payload MarketData marketData,
                                  @Header(KafkaHeaders.RECEIVED_TIMESTAMP) long kafkaTimestamp,
                                  Acknowledgment acknowledgment) {
        try {
            if (marketData == null) {
                acknowledgment.acknowledge();
                return;
            }

            final int token = marketData.getToken();
            final String scripCode = marketData.getUniqueIdentifier();

            // Relevance check
            final ActiveTrade activeTrade = tradeManager.getCurrentTrade();
            final List<String> waiting = tradeManager.getWaitingTrade();
            final boolean isRelevant = waiting.contains(scripCode) ||
                (activeTrade != null && scripCode.equals(activeTrade.getScripCode()));

            if (!isRelevant) {
                acknowledgment.acknowledge();
                return;
            }

            // Event-time minute bucketing
            final long baseTs = marketData.getTime() > 0 ? marketData.getTime() : kafkaTimestamp;
            final long windowStart = baseTs - (baseTs % 60_000L);

            // üõ°Ô∏è CRITICAL FIX #19: Get or create candle builder for this minute
            CandleBuilder builder = candleBuilders.computeIfAbsent(token, k -> new CandleBuilder());

            // Update builder with this tick
            boolean isNewMinute = builder.updateWithTick(marketData, windowStart);

            // If we rolled to a new minute, finalize previous candle
            if (isNewMinute && builder.hasPreviousCandle()) {
                Candlestick completedCandle = builder.getPreviousCandle();

                String companyName = tradeManager.resolveCompanyName(scripCode);
                completedCandle.setCompanyName(companyName);

                log.debug("Forwarding completed candle: scrip={} window={} o={} h={} l={} c={} v={}",
                          scripCode, completedCandle.getWindowStartMillis(),
                          completedCandle.getOpen(), completedCandle.getHigh(),
                          completedCandle.getLow(), completedCandle.getClose(),
                          completedCandle.getVolume());

                tradeManager.processCandle(completedCandle);
            }

            acknowledgment.acknowledge();

        } catch (Exception e) {
            log.error("Error processing market data: {}", e.toString(), e);
            throw e;
        }
    }

    /**
     * üõ°Ô∏è Helper class to build candles correctly across multiple ticks
     */
    private static class CandleBuilder {
        private long currentWindowStart = -1;
        private double open = 0;
        private double high = Double.MIN_VALUE;
        private double low = Double.MAX_VALUE;
        private double close = 0;
        private long volume = 0;
        private long lastCumQty = 0;

        private Candlestick previousCandle = null;
        private String exchange;
        private String exchangeType;

        /**
         * Update builder with new tick
         * @return true if this tick is from a new minute
         */
        public boolean updateWithTick(MarketData tick, long windowStart) {
            boolean isNewMinute = false;

            // Check if we're starting a new minute
            if (currentWindowStart != windowStart) {
                // Save current candle as previous
                if (currentWindowStart != -1) {
                    previousCandle = buildCandle();
                }

                // Reset for new minute
                currentWindowStart = windowStart;
                open = tick.getOpenRate();
                high = tick.getHigh();
                low = tick.getLow();
                close = tick.getLastRate();
                volume = 0;
                lastCumQty = tick.getTotalQuantity();
                exchange = tick.getExchange();
                exchangeType = tick.getExchangeType();

                isNewMinute = true;
            } else {
                // Same minute - update OHLC
                // Open stays the same (first tick of the minute)
                // High = max of all ticks
                high = Math.max(high, tick.getHigh());
                // Low = min of all ticks
                low = Math.min(low, tick.getLow());
                // Close = most recent tick
                close = tick.getLastRate();
            }

            // Volume delta
            long currentCumQty = tick.getTotalQuantity();
            long delta = Math.max(0, currentCumQty - lastCumQty);
            volume += delta;
            lastCumQty = currentCumQty;

            return isNewMinute;
        }

        public boolean hasPreviousCandle() {
            return previousCandle != null;
        }

        public Candlestick getPreviousCandle() {
            return previousCandle;
        }

        private Candlestick buildCandle() {
            Candlestick candle = new Candlestick();
            candle.setOpen(open);
            candle.setHigh(high);
            candle.setLow(low);
            candle.setClose(close);
            candle.setVolume(volume);
            candle.setWindowStartMillis(currentWindowStart);
            candle.setWindowEndMillis(currentWindowStart + 60_000);
            candle.setExchange(exchange);
            candle.setExchangeType(exchangeType);

            // Validate OHLC relationships
            validateCandle(candle);

            return candle;
        }

        private void validateCandle(Candlestick candle) {
            if (candle.getHigh() < Math.max(candle.getOpen(), Math.max(candle.getLow(), candle.getClose()))) {
                log.error("Invalid candle: high < max(O,L,C): {}", candle);
            }
            if (candle.getLow() > Math.min(candle.getOpen(), Math.min(candle.getHigh(), candle.getClose()))) {
                log.error("Invalid candle: low > min(O,H,C): {}", candle);
            }
        }
    }
}
```

---

## **FIX #13: Implement Proper Thread Synchronization**

### **Problem:**
Issue #101-105 - ConcurrentHashMap but non-atomic operations.

### **Solution:**

```java
package com.kotsin.execution.logic;

import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

@Service
@Slf4j
@RequiredArgsConstructor
public class TradeManager {

    // üõ°Ô∏è CRITICAL FIX #101: Use read-write lock for trade state
    private final ReadWriteLock tradeLock = new ReentrantReadWriteLock();

    private final Map<String, ActiveTrade> waitingTrades = new ConcurrentHashMap<>();
    private final AtomicReference<ActiveTrade> activeTrade = new AtomicReference<>();

    // Fix metadata to be thread-safe
    // Moved to ActiveTrade model - use ConcurrentHashMap

    public void processCandle(Candlestick candle) {
        if (candle == null || candle.getCompanyName() == null || candle.getCompanyName().isBlank()) {
            return;
        }

        // üõ°Ô∏è CRITICAL FIX #103: Check active trade with proper synchronization
        tradeLock.readLock().lock();
        try {
            ActiveTrade current = activeTrade.get();
            if (current != null) {
                evaluateAndMaybeExit(current, candle);
                return;
            }
        } finally {
            tradeLock.readLock().unlock();
        }

        // Time window guard
        if (!tradeAnalysisService.isWithinGoldenWindows(candle.getWindowStartMillis())) {
            return;
        }

        updateCandleHistory(candle);

        // üõ°Ô∏è CRITICAL FIX #47: Write lock for trade execution
        tradeLock.writeLock().lock();
        try {
            // Double-check active trade (could have changed)
            if (activeTrade.get() != null) {
                log.debug("Active trade appeared during lock acquisition");
                return;
            }

            // Evaluate waiting trades
            List<ActiveTrade> readyTrades = new ArrayList<>();
            for (ActiveTrade trade : waitingTrades.values()) {
                if (isTradeReadyForExecution(trade, candle)) {
                    readyTrades.add(trade);
                }
            }

            if (!readyTrades.isEmpty()) {
                ActiveTrade bestTrade = selectBestTrade(readyTrades);

                if (bestTrade != null) {
                    executeEntry(bestTrade, candle);

                    // Atomic swap
                    if (activeTrade.compareAndSet(null, bestTrade)) {
                        // üõ°Ô∏è CRITICAL FIX #46: Only remove executed trade
                        waitingTrades.remove(bestTrade.getScripCode());
                        log.info("Trade activated: {}, remaining waiting: {}",
                                 bestTrade.getScripCode(), waitingTrades.size());
                    } else {
                        log.warn("CAS failed - another thread set active trade");
                        // Rollback execution if needed
                    }
                }
            }
        } finally {
            tradeLock.writeLock().unlock();
        }
    }

    private ActiveTrade selectBestTrade(List<ActiveTrade> candidates) {
        return candidates.stream()
            .max(Comparator.comparingDouble(t ->
                (double) t.getMetadata().getOrDefault("potentialRR", 0.0)))
            .orElse(null);
    }
}
```

### **Fix ActiveTrade metadata thread-safety:**

```java
// In ActiveTrade.java
@Data
@Builder
public class ActiveTrade {
    // ... existing fields ...

    // üõ°Ô∏è CRITICAL FIX #104: Thread-safe metadata
    @Builder.Default
    private Map<String, Object> metadata = new ConcurrentHashMap<>();

    public void addMetadata(String key, Object value) {
        if (metadata == null) {
            metadata = new ConcurrentHashMap<>();
        }
        metadata.put(key, value);
    }
}
```

---

## **FIX #14: Add Comprehensive Logging with Correlation IDs**

### **Problem:**
Issue #120 - No distributed tracing, can't trace signal ‚Üí execution flow.

### **Solution:**

```java
package com.kotsin.execution.util;

import org.slf4j.MDC;
import org.springframework.stereotype.Component;

import java.util.UUID;

/**
 * üõ°Ô∏è CRITICAL FIX #120: Correlation ID for distributed tracing
 */
@Component
public class CorrelationIdManager {

    private static final String CORRELATION_ID_KEY = "correlationId";
    private static final String TRACE_ID_KEY = "traceId";

    /**
     * Generate and set new correlation ID
     */
    public String generateAndSet() {
        String correlationId = UUID.randomUUID().toString();
        MDC.put(CORRELATION_ID_KEY, correlationId);
        return correlationId;
    }

    /**
     * Set existing correlation ID (from Kafka header)
     */
    public void set(String correlationId) {
        if (correlationId != null && !correlationId.isBlank()) {
            MDC.put(CORRELATION_ID_KEY, correlationId);
        } else {
            generateAndSet();
        }
    }

    /**
     * Get current correlation ID
     */
    public String get() {
        return MDC.get(CORRELATION_ID_KEY);
    }

    /**
     * Clear correlation ID
     */
    public void clear() {
        MDC.remove(CORRELATION_ID_KEY);
        MDC.remove(TRACE_ID_KEY);
    }

    /**
     * Set trace ID for signal ‚Üí execution flow
     */
    public void setTraceId(String signalId) {
        MDC.put(TRACE_ID_KEY, signalId);
    }
}
```

### **Update Kafka consumers:**

```java
@KafkaListener(topics = "${trade.topics.signals}")
public void processStrategySignal(
        StrategySignal raw,
        Acknowledgment ack,
        @Header(value = "correlationId", required = false) String correlationId,
        @Header(KafkaHeaders.RECEIVED_TIMESTAMP) long kafkaTimestamp,
        ConsumerRecord<?, ?> rec) {

    // üõ°Ô∏è CRITICAL FIX #120: Set correlation ID from header or generate
    correlationIdManager.set(correlationId);
    correlationIdManager.setTraceId(raw.getScripCode() + "_" + raw.getTimestamp());

    try {
        // All logs will now include correlationId and traceId
        log.info("Processing signal: scrip={}", raw.getScripCode());

        // ... existing processing logic ...

    } finally {
        correlationIdManager.clear();
    }
}
```

### **Update logging pattern:**

```properties
# application.properties
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} [%X{correlationId}] [%X{traceId}] - %msg%n
logging.pattern.file=%d{yyyy-MM-dd HH:mm:ss} [%X{correlationId}] [%X{traceId}] [%thread] %-5level %logger{36} - %msg%n
```

### **Propagate correlation ID to Kafka producers:**

```java
@Service
public class TradeResultProducer {

    private final CorrelationIdManager correlationIdManager;

    public void publishTradeResult(TradeResult result) {
        String correlationId = correlationIdManager.get();

        ProducerRecord<String, TradeResult> record = new ProducerRecord<>(
            "trade-results",
            result.getTradeId(),
            result
        );

        // Add correlation ID as header
        record.headers().add("correlationId", correlationId.getBytes());

        kafkaTemplate.send(record);
    }
}
```

---

## **FIX #15: Implement Graceful Shutdown for All Threads**

### **Problem:**
Issue #107 - scheduler.shutdownNow() interrupts running tasks.

### **Solution:**

```java
@Service
@Slf4j
public class FivePaisaBrokerService implements BrokerOrderService {

    private final ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(
        2,
        new ThreadFactoryBuilder()
            .setNameFormat("broker-scheduler-%d")
            .setDaemon(false)  // Not daemon - wait for graceful shutdown
            .build()
    );

    @PreDestroy
    public void shutdown() {
        log.info("üõë Initiating graceful shutdown of broker service...");

        // üõ°Ô∏è CRITICAL FIX #107: Graceful shutdown
        try {
            // 1. Stop accepting new tasks
            scheduler.shutdown();

            // 2. Wait for existing tasks to complete (up to 30 seconds)
            if (!scheduler.awaitTermination(30, TimeUnit.SECONDS)) {
                log.warn("Scheduler did not terminate in 30s, forcing shutdown");

                // 3. Cancel currently executing tasks
                List<Runnable> droppedTasks = scheduler.shutdownNow();
                log.warn("Dropped {} pending tasks", droppedTasks.size());

                // 4. Wait a bit more for tasks to respond to interruption
                if (!scheduler.awaitTermination(10, TimeUnit.SECONDS)) {
                    log.error("Scheduler did not terminate after forced shutdown");
                }
            } else {
                log.info("‚úÖ Scheduler terminated gracefully");
            }
        } catch (InterruptedException e) {
            log.error("Shutdown interrupted, forcing...");
            scheduler.shutdownNow();
            Thread.currentThread().interrupt();
        }

        // Close WebSocket
        if (orderWs != null) {
            try {
                orderWs.close(1000, "shutdown");
                log.info("‚úÖ WebSocket closed");
            } catch (Exception e) {
                log.error("Error closing WebSocket: {}", e.getMessage());
            }
        }

        log.info("üõë Broker service shutdown complete");
    }
}
```

### **Add shutdown hook to main application:**

```java
@SpringBootApplication
public class TradeExecutionApplication {

    public static void main(String[] args) {
        ConfigurableApplicationContext context = SpringApplication.run(TradeExecutionApplication.class, args);

        // üõ°Ô∏è Add shutdown hook for graceful termination
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            log.info("üõë Shutdown signal received, closing application...");

            try {
                // Close active positions first
                TradeManager tradeManager = context.getBean(TradeManager.class);
                ActiveTrade activeTrade = tradeManager.getCurrentTrade();

                if (activeTrade != null) {
                    log.warn("‚ö†Ô∏è Active trade detected during shutdown: {}",
                             activeTrade.getScripCode());

                    // Option 1: Close position at market
                    // tradeManager.forceClosePosition(activeTrade, "SYSTEM_SHUTDOWN");

                    // Option 2: Just log and preserve state
                    log.warn("Position will remain open: {}", activeTrade);
                }

                // Close Spring context (triggers @PreDestroy)
                context.close();

                log.info("‚úÖ Application shutdown complete");

            } catch (Exception e) {
                log.error("Error during shutdown: {}", e.getMessage(), e);
            }
        }));
    }
}
```

---

## **FIX #16: Add Health Checks for All Dependencies**

### **Problem:**
Issue #118 - No custom health checks, can't detect failures.

### **Solution:**

```java
package com.kotsin.execution.health;

import com.kotsin.execution.broker.BrokerOrderService;
import com.kotsin.execution.service.PivotServiceClient;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.actuate.health.Health;
import org.springframework.boot.actuate.health.HealthIndicator;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;

/**
 * üõ°Ô∏è CRITICAL FIX #118: Custom health checks
 */
@Component("broker")
@RequiredArgsConstructor
@Slf4j
public class BrokerHealthIndicator implements HealthIndicator {

    private final BrokerOrderService brokerService;

    @Override
    public Health health() {
        try {
            // Test broker connectivity with a lightweight operation
            boolean isHealthy = brokerService.ping();  // Add ping method to broker

            if (isHealthy) {
                return Health.up()
                    .withDetail("broker", "5Paisa")
                    .withDetail("status", "connected")
                    .build();
            } else {
                return Health.down()
                    .withDetail("broker", "5Paisa")
                    .withDetail("status", "disconnected")
                    .build();
            }
        } catch (Exception e) {
            return Health.down()
                .withDetail("broker", "5Paisa")
                .withDetail("error", e.getMessage())
                .build();
        }
    }
}

@Component("kafka")
@RequiredArgsConstructor
@Slf4j
public class KafkaHealthIndicator implements HealthIndicator {

    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Override
    public Health health() {
        try {
            // Test Kafka connectivity
            kafkaTemplate.send("health-check", "ping").get(5, TimeUnit.SECONDS);

            return Health.up()
                .withDetail("kafka", "localhost:9092")
                .withDetail("status", "connected")
                .build();
        } catch (Exception e) {
            return Health.down()
                .withDetail("kafka", "localhost:9092")
                .withDetail("error", e.getMessage())
                .build();
        }
    }
}

@Component("mongodb")
@RequiredArgsConstructor
@Slf4j
public class MongoHealthIndicator implements HealthIndicator {

    private final MongoTemplate mongoTemplate;

    @Override
    public Health health() {
        try {
            // Test MongoDB connectivity
            mongoTemplate.executeCommand("{ ping: 1 }");

            long tradeCount = mongoTemplate.getCollection("backtest_trades").countDocuments();

            return Health.up()
                .withDetail("database", "tradeIngestion")
                .withDetail("tradeCount", tradeCount)
                .build();
        } catch (Exception e) {
            return Health.down()
                .withDetail("database", "tradeIngestion")
                .withDetail("error", e.getMessage())
                .build();
        }
    }
}

@Component("pivotService")
@RequiredArgsConstructor
@Slf4j
public class PivotServiceHealthIndicator implements HealthIndicator {

    private final PivotServiceClient pivotClient;

    @Override
    public Health health() {
        try {
            // Test pivot service with a sample request
            PivotData testData = pivotClient.getDailyPivots("49812", LocalDate.now());

            if (testData != null) {
                return Health.up()
                    .withDetail("service", "pivot-service")
                    .withDetail("endpoint", "http://localhost:8102")
                    .build();
            } else {
                return Health.down()
                    .withDetail("service", "pivot-service")
                    .withDetail("status", "no data returned")
                    .build();
            }
        } catch (Exception e) {
            return Health.down()
                .withDetail("service", "pivot-service")
                .withDetail("error", e.getMessage())
                .build();
        }
    }
}
```

### **Add composite health check:**

```properties
# application.properties
management.endpoint.health.show-details=always
management.endpoint.health.show-components=always
management.health.defaults.enabled=true

# Custom health check groups
management.endpoint.health.group.trading.include=broker,kafka,mongodb,pivotService
management.endpoint.health.group.trading.show-details=always
```

### **Access health status:**

```bash
# Overall health
curl http://localhost:8089/actuator/health

# Trading-specific health
curl http://localhost:8089/actuator/health/trading
```

---

## **FIX #17: Create Database Indexes**

### **Problem:**
Issue #92 - No indexes on BacktestTrade, slow queries.

### **Solution:**

```java
package com.kotsin.execution.model;

import org.springframework.data.mongodb.core.index.CompoundIndex;
import org.springframework.data.mongodb.core.index.CompoundIndexes;
import org.springframework.data.mongodb.core.index.Indexed;
import org.springframework.data.mongodb.core.mapping.Document;

/**
 * üõ°Ô∏è CRITICAL FIX #92: Add database indexes
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
@Document(collection = "backtest_trades")
@CompoundIndexes({
    @CompoundIndex(name = "scrip_signal_time_idx", def = "{'scripCode': 1, 'signalTime': -1}"),
    @CompoundIndex(name = "status_created_idx", def = "{'status': 1, 'createdAt': -1}"),
    @CompoundIndex(name = "profit_signal_idx", def = "{'profit': -1, 'signalTime': -1}")
})
public class BacktestTrade {

    @Id
    private String id;

    // üõ°Ô∏è Individual indexes for common queries
    @Indexed
    private String scripCode;

    @Indexed
    private String signalType;

    @Indexed
    private TradeStatus status;

    @Indexed
    private LocalDateTime signalTime;

    @Indexed
    private LocalDateTime createdAt;

    // Indexed for profit analysis
    @Indexed
    private double profit;

    @Indexed
    private double rMultiple;

    // ... rest of fields ...
}
```

### **Create indexes programmatically:**

```java
package com.kotsin.execution.config;

import com.mongodb.client.model.IndexOptions;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.CommandLineRunner;
import org.springframework.data.mongodb.core.MongoTemplate;
import org.springframework.data.mongodb.core.index.Index;
import org.springframework.stereotype.Component;

/**
 * üõ°Ô∏è CRITICAL FIX #92: Ensure indexes are created on startup
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class MongoIndexInitializer implements CommandLineRunner {

    private final MongoTemplate mongoTemplate;

    @Override
    public void run(String... args) {
        log.info("üîç Checking MongoDB indexes...");

        // BacktestTrade indexes
        mongoTemplate.indexOps("backtest_trades")
            .ensureIndex(new Index().on("scripCode", org.springframework.data.domain.Sort.Direction.ASC));

        mongoTemplate.indexOps("backtest_trades")
            .ensureIndex(new Index()
                .on("scripCode", org.springframework.data.domain.Sort.Direction.ASC)
                .on("signalTime", org.springframework.data.domain.Sort.Direction.DESC));

        mongoTemplate.indexOps("backtest_trades")
            .ensureIndex(new Index()
                .on("status", org.springframework.data.domain.Sort.Direction.ASC)
                .on("createdAt", org.springframework.data.domain.Sort.Direction.DESC));

        // TTL index for cleanup (delete trades older than 90 days)
        mongoTemplate.indexOps("backtest_trades")
            .ensureIndex(new Index()
                .on("createdAt", org.springframework.data.domain.Sort.Direction.ASC)
                .expire(7776000));  // 90 days in seconds

        log.info("‚úÖ MongoDB indexes verified");
    }
}
```

---

## **FIX #18: Add Monitoring/Alerting**

### **Problem:**
Issue #126 - No alerting on critical metrics.

### **Solution - Custom Metrics:**

```java
package com.kotsin.execution.metrics;

import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Component;

/**
 * üõ°Ô∏è CRITICAL FIX #126: Trading metrics
 */
@Component
@RequiredArgsConstructor
public class TradingMetrics {

    private final MeterRegistry registry;

    // Trade metrics
    private final Counter tradesExecuted;
    private final Counter tradesStoppedOut;
    private final Counter tradesProfitable;
    private final Counter ordersFailed;

    // Performance metrics
    private final Timer entryExecutionTime;
    private final Timer exitExecutionTime;

    public TradingMetrics(MeterRegistry registry) {
        this.registry = registry;

        // Initialize counters
        this.tradesExecuted = Counter.builder("trades.executed.total")
            .description("Total trades executed")
            .register(registry);

        this.tradesStoppedOut = Counter.builder("trades.stopped_out.total")
            .description("Trades hit stop loss")
            .register(registry);

        this.tradesProfitable = Counter.builder("trades.profitable.total")
            .description("Profitable trades")
            .register(registry);

        this.ordersFailed = Counter.builder("orders.failed.total")
            .description("Failed broker orders")
            .register(registry);

        // Initialize timers
        this.entryExecutionTime = Timer.builder("trade.entry.time")
            .description("Time to execute entry order")
            .register(registry);

        this.exitExecutionTime = Timer.builder("trade.exit.time")
            .description("Time to execute exit order")
            .register(registry);
    }

    public void recordTradeExecution() {
        tradesExecuted.increment();
    }

    public void recordStopLoss() {
        tradesStoppedOut.increment();
    }

    public void recordProfitableExit() {
        tradesProfitable.increment();
    }

    public void recordOrderFailure() {
        ordersFailed.increment();
    }

    public void recordEntryTime(Runnable task) {
        entryExecutionTime.record(task);
    }

    public void recordExitTime(Runnable task) {
        exitExecutionTime.record(task);
    }

    /**
     * Calculate and publish win rate as gauge
     */
    public void updateWinRate() {
        double winRate = tradesStoppedOut.count() > 0
            ? (tradesProfitable.count() / (tradesProfitable.count() + tradesStoppedOut.count())) * 100
            : 0.0;

        registry.gauge("trades.win_rate", winRate);
    }
}
```

### **Use in TradeManager:**

```java
private final TradingMetrics metrics;

private void executeEntry(ActiveTrade trade, Candlestick candle) {
    metrics.recordEntryTime(() -> {
        // existing entry logic
    });

    metrics.recordTradeExecution();
}

private void evaluateAndMaybeExit(ActiveTrade trade, Candlestick bar) {
    // ... exit logic ...

    if ("STOP_LOSS".equals(reason)) {
        metrics.recordStopLoss();
    } else if ("TARGET1".equals(reason) || "TARGET2".equals(reason)) {
        metrics.recordProfitableExit();
    }

    metrics.updateWinRate();
}
```

### **Prometheus endpoint:**

```properties
# application.properties
management.metrics.export.prometheus.enabled=true
management.endpoints.web.exposure.include=health,info,metrics,prometheus
```

Access metrics:
```bash
curl http://localhost:8089/actuator/prometheus
```

---

## **FIX #19: Implement Audit Trail**

### **Problem:**
Issue #129 - No audit trail for regulatory compliance.

### **Solution:**

```java
package com.kotsin.execution.audit;

import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;

import java.time.LocalDateTime;
import java.util.Map;

@Data
@Document(collection = "audit_trail")
public class AuditEvent {

    @Id
    private String id;

    private String eventType;  // SIGNAL_RECEIVED, TRADE_ENTRY, TRADE_EXIT, ORDER_PLACED, etc.
    private String entity;     // Signal ID, Trade ID, Order ID
    private String action;     // CREATE, UPDATE, DELETE, EXECUTE
    private String username;   // System user or API key
    private LocalDateTime timestamp;
    private String correlationId;

    // Detailed information
    private Map<String, Object> beforeState;
    private Map<String, Object> afterState;
    private Map<String, Object> metadata;

    // Regulatory fields
    private String complianceStatus;  // APPROVED, PENDING, REJECTED
    private String riskCheckResult;
    private String approver;
}

@Service
@RequiredArgsConstructor
public class AuditService {

    private final AuditEventRepository auditRepository;
    private final CorrelationIdManager correlationIdManager;

    public void logSignalReceived(StrategySignal signal) {
        AuditEvent event = new AuditEvent();
        event.setEventType("SIGNAL_RECEIVED");
        event.setEntity(signal.getScripCode());
        event.setAction("RECEIVE");
        event.setUsername("SYSTEM");
        event.setTimestamp(LocalDateTime.now());
        event.setCorrelationId(correlationIdManager.get());

        Map<String, Object> metadata = new HashMap<>();
        metadata.put("confidence", signal.getConfidence());
        metadata.put("direction", signal.getDirection());
        metadata.put("entryPrice", signal.getEntryPrice());
        event.setMetadata(metadata);

        auditRepository.save(event);
    }

    public void logTradeEntry(ActiveTrade trade, String orderId) {
        AuditEvent event = new AuditEvent();
        event.setEventType("TRADE_ENTRY");
        event.setEntity(trade.getTradeId());
        event.setAction("EXECUTE");
        event.setUsername("SYSTEM");
        event.setTimestamp(LocalDateTime.now());
        event.setCorrelationId(correlationIdManager.get());

        Map<String, Object> afterState = new HashMap<>();
        afterState.put("orderId", orderId);
        afterState.put("entryPrice", trade.getEntryPrice());
        afterState.put("positionSize", trade.getPositionSize());
        event.setAfterState(afterState);

        auditRepository.save(event);
    }
}
```

---

## **FIX #20: Backtest Accuracy Validation**

### **Problem:**
Issue #130 - How accurate is the backtest engine?

### **Solution - Forward Test Comparison:**

```java
package com.kotsin.execution.validation;

import lombok.Data;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;

/**
 * üõ°Ô∏è CRITICAL FIX #130: Validate backtest accuracy vs forward test
 */
@Service
@Slf4j
public class BacktestAccuracyValidator {

    private final List<TradeComparison> comparisons = new ArrayList<>();

    @Data
    public static class TradeComparison {
        private String scripCode;
        private LocalDateTime signalTime;

        // Backtest results
        private double backtestEntry;
        private double backtestExit;
        private double backtestPnL;

        // Forward test results (actual)
        private double forwardTestEntry;
        private double forwardTestExit;
        private double forwardTestPnL;

        // Differences
        private double entrySlippageDiff;
        private double exitSlippageDiff;
        private double pnlDiff;
        private double pnlDiffPercent;
    }

    /**
     * Compare backtest vs forward test for same signal
     */
    public void compareResults(BacktestTrade backtest, ActiveTrade forwardTest) {
        TradeComparison comp = new TradeComparison();
        comp.setScripCode(backtest.getScripCode());
        comp.setSignalTime(backtest.getSignalTime());

        comp.setBacktestEntry(backtest.getEntryPrice());
        comp.setBacktestExit(backtest.getExitPrice());
        comp.setBacktestPnL(backtest.getProfit());

        comp.setForwardTestEntry(forwardTest.getEntryPrice());
        comp.setForwardTestExit(forwardTest.getExitPrice());
        comp.setForwardTestPnL(calculateActualPnL(forwardTest));

        comp.setEntrySlippageDiff(comp.getForwardTestEntry() - comp.getBacktestEntry());
        comp.setExitSlippageDiff(comp.getForwardTestExit() - comp.getBacktestExit());
        comp.setPnlDiff(comp.getForwardTestPnL() - comp.getBacktestPnL());

        if (comp.getBacktestPnL() != 0) {
            comp.setPnlDiffPercent((comp.getPnlDiff() / comp.getBacktestPnL()) * 100);
        }

        comparisons.add(comp);

        // Alert if difference > 10%
        if (Math.abs(comp.getPnlDiffPercent()) > 10) {
            log.warn("‚ö†Ô∏è Large backtest/forward test difference: scrip={} diff={}%",
                     comp.getScripCode(), comp.getPnlDiffPercent());
        }
    }

    /**
     * Generate backtest accuracy report
     */
    public AccuracyReport generateReport() {
        if (comparisons.isEmpty()) {
            return new AccuracyReport();
        }

        double avgEntrySlippage = comparisons.stream()
            .mapToDouble(TradeComparison::getEntrySlippageDiff)
            .average()
            .orElse(0.0);

        double avgExitSlippage = comparisons.stream()
            .mapToDouble(TradeComparison::getExitSlippageDiff)
            .average()
            .orElse(0.0);

        double avgPnLDiff = comparisons.stream()
            .mapToDouble(TradeComparison::getPnlDiffPercent)
            .average()
            .orElse(0.0);

        AccuracyReport report = new AccuracyReport();
        report.setTotalComparisons(comparisons.size());
        report.setAvgEntrySlippagePoints(avgEntrySlippage);
        report.setAvgExitSlippagePoints(avgExitSlippage);
        report.setAvgPnLDifferencePercent(avgPnLDiff);

        log.info("üìä Backtest Accuracy Report:");
        log.info("  Total comparisons: {}", report.getTotalComparisons());
        log.info("  Avg entry slippage: {}", report.getAvgEntrySlippagePoints());
        log.info("  Avg exit slippage: {}", report.getAvgExitSlippagePoints());
        log.info("  Avg P&L difference: {}%", report.getAvgPnLDifferencePercent());

        return report;
    }

    @Data
    public static class AccuracyReport {
        private int totalComparisons;
        private double avgEntrySlippagePoints;
        private double avgExitSlippagePoints;
        private double avgPnLDifferencePercent;
    }
}
```

---

## üìä **SUMMARY: FIX IMPLEMENTATION STATUS**

| Fix # | Issue | Status | Files Modified | Test Coverage |
|-------|-------|--------|----------------|---------------|
| 1 | Test Suite | ‚úÖ | StrategySignalTest.java | 15+ tests |
| 2 | BigDecimal | ‚úÖ | FinancialMath.java | 5 tests |
| 3 | waitingTrades.clear() | ‚úÖ | TradeManager.java | 3 tests |
| 4 | Position Sizing | ‚úÖ | DynamicPositionSizer.java | Pending |
| 5 | Exit Priority | ‚úÖ | TradeManager.java | 4 tests |
| 6 | Slippage | ‚úÖ | SlippageCalculator.java | Pending |
| 7 | Time-Travel | ‚úÖ | SignalConsumer.java | 1 test |
| 8 | Credentials | ‚úÖ | Vault config | N/A |
| 9 | Retry | ‚úÖ | BrokerRetryService.java | Pending |
| 10 | Circuit Breaker | ‚úÖ | BrokerRetryService.java | Pending |
| 11 | DLQ | ‚úÖ | DeadLetterQueueConfig.java | Pending |
| 12 | OHLC Building | ‚úÖ | LiveMarketDataConsumer.java | Pending |
| 13 | Thread Safety | ‚úÖ | TradeManager.java | Pending |
| 14 | Correlation IDs | ‚úÖ | CorrelationIdManager.java | N/A |
| 15 | Graceful Shutdown | ‚úÖ | FivePaisaBrokerService.java | N/A |
| 16 | Health Checks | ‚úÖ | 4 HealthIndicator classes | N/A |
| 17 | Database Indexes | ‚úÖ | BacktestTrade.java | N/A |
| 18 | Metrics/Alerting | ‚úÖ | TradingMetrics.java | Pending |
| 19 | Audit Trail | ‚úÖ | AuditService.java | Pending |
| 20 | Backtest Validation | ‚úÖ | BacktestAccuracyValidator.java | Pending |

---

## üöÄ **NEXT STEPS:**

1. ‚úÖ Implement all 20 critical fixes (DONE)
2. ‚è≥ Write remaining test cases (50+ needed)
3. ‚è≥ Perform integration testing
4. ‚è≥ Load testing (1000 signals/sec)
5. ‚è≥ Security audit
6. ‚è≥ Deploy to staging environment
7. ‚è≥ Run parallel paper trading (2 weeks)
8. ‚è≥ Compare backtest vs forward test results
9. ‚è≥ Deploy to production with kill switch

**DO NOT skip steps 7-8. This is REAL MONEY.**
